{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7efe4fc-1340-4ad5-ab15-25de6bf4ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "        \n",
    "    return conn[db]\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    db =_connect_mongo(db=db, host=host, port=port, username=username, password=password)\n",
    "    cursor = db[collection].find(query)\n",
    "        \n",
    "    return list(cursor)\n",
    "\n",
    "def _build_vector(event):\n",
    "    # vector_len = 290\n",
    "    # f_vector = np.zeros(vectror_len)\n",
    "    \n",
    "    m_country = assign_country_mapper()\n",
    "    m_event = assign_event_type_mapper()\n",
    "    m_agent_os = assign_agent_os_mapper()\n",
    "    m_agent_name = assign_agent_name_mapper()\n",
    "    \n",
    "    f_vector_evty = np.zeros(4)\n",
    "    f_vector_evty[m_event[event['type']]] = 1\n",
    "    \n",
    "    \n",
    "    dt = datetime.utcfromtimestamp(event['timestamp'])\n",
    "    minutes = (dt.hour * 60) + dt.minute\n",
    "    day = dt.weekday()\n",
    "    f_vector_dt = np.zeros(2)\n",
    "    f_vector_dt[0] = round(minutes / 1439, 5)\n",
    "    f_vector_dt[1] = round(day / 6, 5)\n",
    "    \n",
    "    latitude = event['geoip']['latitude'][0]\n",
    "    longitude = event['geoip']['longitude'][0]\n",
    "    f_vector_geo = np.zeros(2)\n",
    "    f_vector_geo[0]= round((latitude + 90) / 180,5)\n",
    "    f_vector_geo[1]= round((longitude + 180) / 360, 5)\n",
    "    \n",
    "    f_vector_agos = np.zeros(8)\n",
    "    os = event['agent']['os'][0]\n",
    "    if os in m_agent_os:\n",
    "        f_vector_agos[m_agent_os[os]] = 1\n",
    "    else:\n",
    "        f_vector_agos[-1] = 1\n",
    "        \n",
    "    f_vector_agna = np.zeros(26)\n",
    "    name = event['agent']['name'][0]\n",
    "    if name in m_agent_name:\n",
    "        f_vector_agna[m_agent_name[name]] = 1\n",
    "    else:\n",
    "        f_vector_agna[-1] = 1\n",
    "        \n",
    "    f_vector_country = np.zeros(248)\n",
    "    country = event['geoip']['country'][0]\n",
    "    if country in m_country:\n",
    "        f_vector_country[m_country[country]] = 1\n",
    "    else:\n",
    "        f_vector_country[-1] = 1\n",
    "    \n",
    "    # without geo and without country features\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_agos, f_vector_agna])\n",
    "\n",
    "    # without country feature\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna])\n",
    "    \n",
    "    # all features\n",
    "    return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna, f_vector_country])\n",
    "\n",
    "def vectorize(json):\n",
    "    events = sorted(np.concatenate([json['connects'], json['plays'], json['h5liveStats'],json['closes']]), key=lambda d: d['timestamp'])\n",
    "    vector_list = []\n",
    "    if len(events) == 0:\n",
    "        return None\n",
    "    for event in events:\n",
    "        vector_list.append(_build_vector(event))\n",
    "    return np.stack(vector_list)\n",
    "\n",
    "def add_event_types(json):\n",
    "    if 'rtmpStats' in json:\n",
    "        rtmpStats = json['rtmpStats']\n",
    "        for x in rtmpStats:\n",
    "            x['type']='rtmp'\n",
    "    \n",
    "    if 'connects' in json:\n",
    "        connects = json['connects'] \n",
    "        for x in connects:\n",
    "            x['type']='connect'\n",
    "        \n",
    "    if 'plays' in json:\n",
    "        plays = json['plays']\n",
    "        for x in plays:\n",
    "            x['type']='play'\n",
    "            \n",
    "    if 'h5liveStats' in json:\n",
    "        h5liveStats = json['h5liveStats']\n",
    "        for x in h5liveStats:\n",
    "            x['type']='h5live'\n",
    "    \n",
    "    if 'closes' in json:\n",
    "        closes = json['closes']\n",
    "        for x in closes:\n",
    "            x['type']='close'  \n",
    "    \n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed942924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def assign_country_mapper():\n",
    "    with open('../../src/util/country_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "\n",
    "def assign_event_type_mapper():\n",
    "    with open('../../src/util/event_type_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_os_mapper():\n",
    "    with open('../../src/util/agent_os_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_name_mapper():\n",
    "    with open('../../src/util/agent_name_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df62b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from MongoDB\n",
    "misuses = read_mongo('dataset1', 'misuse_data')\n",
    "regulars = read_mongo('dataset1', 'regular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd324fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_misuses = []\n",
    "v_regulars = []\n",
    "\n",
    "for x in misuses:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_misuses.append(y)\n",
    "\n",
    "for x in regulars:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_regulars.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41ade275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  torch.Size([400, 4000, 290])   torch.Size([400, 2])\n",
      "Testing:  torch.Size([273, 3300, 290])   torch.Size([273, 2])\n"
     ]
    }
   ],
   "source": [
    "train_size = (100,300)\n",
    "im = torch.tensor([[1,0], [0,1]])\n",
    "\n",
    "X_train_misuse = v_misuses[:train_size[0]:1]\n",
    "X_train_regular = v_regulars[:train_size[1]:1]\n",
    "X_train_final = X_train_misuse + X_train_regular\n",
    "\n",
    "y_train = np.zeros_like(list(range(len(X_train_final))))\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i >= len(X_train_misuse):\n",
    "        y_train[i] = 1\n",
    "\n",
    "max_length_training = max([arr.shape[0] for arr in X_train_final])\n",
    "padded_train_data = np.array([np.pad(arr,((0, max_length_training - arr.shape[0]), (0,0)), mode='constant') for arr in X_train_final])\n",
    "\n",
    "tensor_train_data = Variable(torch.tensor(padded_train_data, dtype=torch.float64))\n",
    "tensor_train_labels = Variable(im[torch.tensor(y_train)])\n",
    "\n",
    "\n",
    "X_test_misuse = v_misuses[train_size[0]::1]\n",
    "X_test_regular = v_regulars[train_size[1]::1]\n",
    "X_test_final = X_test_misuse + X_test_regular\n",
    "\n",
    "y_test = np.zeros_like(list(range(len(X_test_final))))\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if i >= len(X_test_misuse):\n",
    "        y_test[i] = 1\n",
    "\n",
    "max_length_testing = max([arr.shape[0] for arr in X_test_final])\n",
    "padded_test_data = np.array([np.pad(arr,((0, max_length_testing - arr.shape[0]), (0,0)), mode='constant') for arr in X_test_final])\n",
    "\n",
    "tensor_test_data = Variable(torch.tensor(padded_test_data, dtype=torch.float64))\n",
    "tensor_test_labels = Variable(im[torch.tensor(y_test)])\n",
    "\n",
    "print('Training: ', tensor_train_data.shape, ' ', tensor_train_labels.shape)\n",
    "print('Testing: ', tensor_test_data.shape, ' ', tensor_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c12eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = tensor_train_data.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, seq_length):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # use last output of the last lstm layer\n",
    "        out = lstm_out[:, -1, :]\n",
    "    \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.sigm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, tensor_train_data.shape[0]).to(device)\n",
    "\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8df5a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/25], Loss: 1.2367\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(message)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m logging:\n\u001b[1;32m     29\u001b[0m     \u001b[39m# log results\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./logs/output.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m         f\u001b[39m.\u001b[39mwrite(message\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/output.txt'"
     ]
    }
   ],
   "source": [
    "logging = True\n",
    "\n",
    "# set train mode \n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "total_step = len(tensor_train_data) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(tensor_train_data), batch_size):\n",
    "        \n",
    "        # Mini-batch data\n",
    "        batch_inputs = tensor_train_data[i:i+batch_size, :, :].float()\n",
    "        batch_labels = tensor_train_labels[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        loss = loss_func(outputs.float(), batch_labels.float())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        if (i+1) % 1 == 0:\n",
    "            message = 'Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i//batch_size+1, total_step, loss.item())\n",
    "            print(message)\n",
    "            if logging:\n",
    "                # log results\n",
    "                with open('./logs/output.txt', 'a') as f:\n",
    "                    f.write(message+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26cf6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "# set evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tensor_test_data.float())\n",
    "\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "acc= (preds == tensor_test_labels.argmax(1)).sum().item() / tensor_test_labels.shape[0]\n",
    "\n",
    "\n",
    "print(\"accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "454a425d5820fe3d5235a5c44cbe7f0b11db398b8989037c90b6a1734bb532fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
