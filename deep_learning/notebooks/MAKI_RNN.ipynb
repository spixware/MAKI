{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7efe4fc-1340-4ad5-ab15-25de6bf4ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "        \n",
    "    return conn[db]\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    db =_connect_mongo(db=db, host=host, port=port, username=username, password=password)\n",
    "    cursor = db[collection].find(query)\n",
    "        \n",
    "    return list(cursor)\n",
    "\n",
    "def _build_vector(event):\n",
    "    # vector_len = 290\n",
    "    # f_vector = np.zeros(vectror_len)\n",
    "    \n",
    "    m_country = assign_country_mapper()\n",
    "    m_event = assign_event_type_mapper()\n",
    "    m_agent_os = assign_agent_os_mapper()\n",
    "    m_agent_name = assign_agent_name_mapper()\n",
    "    \n",
    "    f_vector_evty = np.zeros(4)\n",
    "    f_vector_evty[m_event[event['type']]] = 1\n",
    "    \n",
    "    \n",
    "    dt = datetime.utcfromtimestamp(event['timestamp'])\n",
    "    minutes = (dt.hour * 60) + dt.minute\n",
    "    day = dt.weekday()\n",
    "    f_vector_dt = np.zeros(2)\n",
    "    f_vector_dt[0] = round(minutes / 1439, 5)\n",
    "    f_vector_dt[1] = round(day / 6, 5)\n",
    "    \n",
    "    latitude = event['geoip']['latitude'][0]\n",
    "    longitude = event['geoip']['longitude'][0]\n",
    "    f_vector_geo = np.zeros(2)\n",
    "    f_vector_geo[0]= round((latitude + 90) / 180,5)\n",
    "    f_vector_geo[1]= round((longitude + 180) / 360, 5)\n",
    "    \n",
    "    f_vector_agos = np.zeros(8)\n",
    "    os = event['agent']['os'][0]\n",
    "    if os in m_agent_os:\n",
    "        f_vector_agos[m_agent_os[os]] = 1\n",
    "    else:\n",
    "        f_vector_agos[-1] = 1\n",
    "        \n",
    "    f_vector_agna = np.zeros(26)\n",
    "    name = event['agent']['name'][0]\n",
    "    if name in m_agent_name:\n",
    "        f_vector_agna[m_agent_name[name]] = 1\n",
    "    else:\n",
    "        f_vector_agna[-1] = 1\n",
    "        \n",
    "    f_vector_country = np.zeros(248)\n",
    "    country = event['geoip']['country'][0]\n",
    "    if country in m_country:\n",
    "        f_vector_country[m_country[country]] = 1\n",
    "    else:\n",
    "        f_vector_country[-1] = 1\n",
    "    \n",
    "    # without geo and without country features\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_agos, f_vector_agna])\n",
    "\n",
    "    # without country feature\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna])\n",
    "    \n",
    "    # all features\n",
    "    return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna, f_vector_country])\n",
    "\n",
    "def vectorize(json):\n",
    "    events = sorted(np.concatenate([json['connects'], json['plays'], json['h5liveStats'],json['closes']]), key=lambda d: d['timestamp'])\n",
    "    vector_list = []\n",
    "    if len(events) == 0:\n",
    "        return None\n",
    "    for event in events:\n",
    "        vector_list.append(_build_vector(event))\n",
    "    return np.stack(vector_list)\n",
    "\n",
    "def add_event_types(json):\n",
    "    if 'rtmpStats' in json:\n",
    "        rtmpStats = json['rtmpStats']\n",
    "        for x in rtmpStats:\n",
    "            x['type']='rtmp'\n",
    "    \n",
    "    if 'connects' in json:\n",
    "        connects = json['connects'] \n",
    "        for x in connects:\n",
    "            x['type']='connect'\n",
    "        \n",
    "    if 'plays' in json:\n",
    "        plays = json['plays']\n",
    "        for x in plays:\n",
    "            x['type']='play'\n",
    "            \n",
    "    if 'h5liveStats' in json:\n",
    "        h5liveStats = json['h5liveStats']\n",
    "        for x in h5liveStats:\n",
    "            x['type']='h5live'\n",
    "    \n",
    "    if 'closes' in json:\n",
    "        closes = json['closes']\n",
    "        for x in closes:\n",
    "            x['type']='close'  \n",
    "    \n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed942924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def assign_country_mapper():\n",
    "    with open('../../src/util/country_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "\n",
    "def assign_event_type_mapper():\n",
    "    with open('../../src/util/event_type_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_os_mapper():\n",
    "    with open('../../src/util/agent_os_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_name_mapper():\n",
    "    with open('../../src/util/agent_name_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df62b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from MongoDB\n",
    "misuses = read_mongo('dataset1', 'misuse_data')\n",
    "regulars = read_mongo('dataset1', 'regular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd324fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_misuses = []\n",
    "v_regulars = []\n",
    "\n",
    "for x in misuses:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_misuses.append(y)\n",
    "\n",
    "for x in regulars:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_regulars.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ade275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  torch.Size([642, 4000, 290])   torch.Size([642, 2])\n",
      "Testing:  torch.Size([61, 3300, 290])   torch.Size([61, 2])\n"
     ]
    }
   ],
   "source": [
    "train_size = (144,528)\n",
    "im = torch.tensor([[1,0], [0,1]])\n",
    "\n",
    "X_train_misuse = v_misuses[:train_size[0]:1]\n",
    "X_train_regular = v_regulars[:train_size[1]:1]\n",
    "X_train_final = X_train_misuse + X_train_regular\n",
    "\n",
    "y_train = np.zeros_like(list(range(len(X_train_final))))\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i >= len(X_train_misuse):\n",
    "        y_train[i] = 1\n",
    "\n",
    "max_length_training = max([arr.shape[0] for arr in X_train_final])\n",
    "padded_train_data = np.array([np.pad(arr,((0, max_length_training - arr.shape[0]), (0,0)), mode='constant') for arr in X_train_final])\n",
    "\n",
    "tensor_train_data = Variable(torch.tensor(padded_train_data, dtype=torch.float64))\n",
    "tensor_train_labels = Variable(im[torch.tensor(y_train)])\n",
    "\n",
    "\n",
    "X_test_misuse = v_misuses[train_size[0]::1]\n",
    "X_test_regular = v_regulars[train_size[1]::1]\n",
    "X_test_final = X_test_misuse + X_test_regular\n",
    "\n",
    "y_test = np.zeros_like(list(range(len(X_test_final))))\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if i >= len(X_test_misuse):\n",
    "        y_test[i] = 1\n",
    "\n",
    "max_length_testing = max([arr.shape[0] for arr in X_test_final])\n",
    "padded_test_data = np.array([np.pad(arr,((0, max_length_testing - arr.shape[0]), (0,0)), mode='constant') for arr in X_test_final])\n",
    "\n",
    "tensor_test_data = Variable(torch.tensor(padded_test_data, dtype=torch.float64))\n",
    "tensor_test_labels = Variable(im[torch.tensor(y_test)])\n",
    "\n",
    "print('Training: ', tensor_train_data.shape, ' ', tensor_train_labels.shape)\n",
    "print('Testing: ', tensor_test_data.shape, ' ', tensor_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c12eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = tensor_train_data.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.003\n",
    "batch_size = 16\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, seq_length):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # use last output of the last lstm layer\n",
    "        out = lstm_out[:, -1, :]\n",
    "    \n",
    "        # first fully connected layer\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        # second fully connected layer\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        # third fully connected layer\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        out = self.sigm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, tensor_train_data.shape[0]).to(device)\n",
    "\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#define for filename\n",
    "num_fclayers = 3\n",
    "last_func = 'sigm'\n",
    "l_func = 'BCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8df5a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/40], Loss: 0.6838\n",
      "Epoch [1/10], Batch [2/40], Loss: 0.6039\n",
      "Epoch [1/10], Batch [3/40], Loss: 0.5203\n",
      "Epoch [1/10], Batch [4/40], Loss: 0.4238\n",
      "Epoch [1/10], Batch [5/40], Loss: 0.3109\n",
      "Epoch [1/10], Batch [6/40], Loss: 0.1873\n",
      "Epoch [1/10], Batch [7/40], Loss: 0.0753\n",
      "Epoch [1/10], Batch [8/40], Loss: 0.0112\n",
      "Epoch [1/10], Batch [9/40], Loss: 0.0002\n",
      "Epoch [1/10], Batch [10/40], Loss: 14.6107\n",
      "Epoch [1/10], Batch [11/40], Loss: 11.0913\n",
      "Epoch [1/10], Batch [12/40], Loss: 7.0207\n",
      "Epoch [1/10], Batch [13/40], Loss: 4.1438\n",
      "Epoch [1/10], Batch [14/40], Loss: 2.6834\n",
      "Epoch [1/10], Batch [15/40], Loss: 1.9237\n",
      "Epoch [1/10], Batch [16/40], Loss: 1.4922\n",
      "Epoch [1/10], Batch [17/40], Loss: 1.2291\n",
      "Epoch [1/10], Batch [18/40], Loss: 1.0591\n",
      "Epoch [1/10], Batch [19/40], Loss: 0.9436\n",
      "Epoch [1/10], Batch [20/40], Loss: 0.8618\n",
      "Epoch [1/10], Batch [21/40], Loss: 0.8018\n",
      "Epoch [1/10], Batch [22/40], Loss: 0.7562\n",
      "Epoch [1/10], Batch [23/40], Loss: 0.7207\n",
      "Epoch [1/10], Batch [24/40], Loss: 0.6920\n",
      "Epoch [1/10], Batch [25/40], Loss: 0.6684\n",
      "Epoch [1/10], Batch [26/40], Loss: 0.6483\n",
      "Epoch [1/10], Batch [27/40], Loss: 0.6307\n",
      "Epoch [1/10], Batch [28/40], Loss: 0.6149\n",
      "Epoch [1/10], Batch [29/40], Loss: 0.6004\n",
      "Epoch [1/10], Batch [30/40], Loss: 0.5867\n",
      "Epoch [1/10], Batch [31/40], Loss: 0.5735\n",
      "Epoch [1/10], Batch [32/40], Loss: 0.5605\n",
      "Epoch [1/10], Batch [33/40], Loss: 0.5476\n",
      "Epoch [1/10], Batch [34/40], Loss: 0.5345\n",
      "Epoch [1/10], Batch [35/40], Loss: 0.5212\n",
      "Epoch [1/10], Batch [36/40], Loss: 0.5074\n",
      "Epoch [1/10], Batch [37/40], Loss: 0.4932\n",
      "Epoch [1/10], Batch [38/40], Loss: 0.4783\n",
      "Epoch [1/10], Batch [39/40], Loss: 0.4629\n",
      "Epoch [1/10], Batch [40/40], Loss: 0.4467\n",
      "Epoch [1/10], Batch [41/40], Loss: 0.4298\n",
      "Epoch [2/10], Batch [1/40], Loss: 1.1041\n",
      "Epoch [2/10], Batch [2/40], Loss: 1.1144\n",
      "Epoch [2/10], Batch [3/40], Loss: 1.1232\n",
      "Epoch [2/10], Batch [4/40], Loss: 1.1214\n",
      "Epoch [2/10], Batch [5/40], Loss: 1.1105\n",
      "Epoch [2/10], Batch [6/40], Loss: 1.0925\n",
      "Epoch [2/10], Batch [7/40], Loss: 1.0694\n",
      "Epoch [2/10], Batch [8/40], Loss: 1.0428\n",
      "Epoch [2/10], Batch [9/40], Loss: 1.0143\n",
      "Epoch [2/10], Batch [10/40], Loss: 0.4690\n",
      "Epoch [2/10], Batch [11/40], Loss: 0.4821\n",
      "Epoch [2/10], Batch [12/40], Loss: 0.4920\n",
      "Epoch [2/10], Batch [13/40], Loss: 0.4992\n",
      "Epoch [2/10], Batch [14/40], Loss: 0.5041\n",
      "Epoch [2/10], Batch [15/40], Loss: 0.5069\n",
      "Epoch [2/10], Batch [16/40], Loss: 0.5079\n",
      "Epoch [2/10], Batch [17/40], Loss: 0.5073\n",
      "Epoch [2/10], Batch [18/40], Loss: 0.5052\n",
      "Epoch [2/10], Batch [19/40], Loss: 0.5017\n",
      "Epoch [2/10], Batch [20/40], Loss: 0.4971\n",
      "Epoch [2/10], Batch [21/40], Loss: 0.4912\n",
      "Epoch [2/10], Batch [22/40], Loss: 0.4843\n",
      "Epoch [2/10], Batch [23/40], Loss: 0.4764\n",
      "Epoch [2/10], Batch [24/40], Loss: 0.4675\n",
      "Epoch [2/10], Batch [25/40], Loss: 0.4576\n",
      "Epoch [2/10], Batch [26/40], Loss: 0.4467\n",
      "Epoch [2/10], Batch [27/40], Loss: 0.4350\n",
      "Epoch [2/10], Batch [28/40], Loss: 0.4224\n",
      "Epoch [2/10], Batch [29/40], Loss: 0.4089\n",
      "Epoch [2/10], Batch [30/40], Loss: 0.3946\n",
      "Epoch [2/10], Batch [31/40], Loss: 0.3795\n",
      "Epoch [2/10], Batch [32/40], Loss: 0.3637\n",
      "Epoch [2/10], Batch [33/40], Loss: 0.3472\n",
      "Epoch [2/10], Batch [34/40], Loss: 0.3300\n",
      "Epoch [2/10], Batch [35/40], Loss: 0.3122\n",
      "Epoch [2/10], Batch [36/40], Loss: 0.2940\n",
      "Epoch [2/10], Batch [37/40], Loss: 0.2755\n",
      "Epoch [2/10], Batch [38/40], Loss: 0.2567\n",
      "Epoch [2/10], Batch [39/40], Loss: 0.2377\n",
      "Epoch [2/10], Batch [40/40], Loss: 0.2189\n",
      "Epoch [2/10], Batch [41/40], Loss: 0.2003\n",
      "Epoch [3/10], Batch [1/40], Loss: 1.7835\n",
      "Epoch [3/10], Batch [2/40], Loss: 1.8271\n",
      "Epoch [3/10], Batch [3/40], Loss: 1.8124\n",
      "Epoch [3/10], Batch [4/40], Loss: 1.7594\n",
      "Epoch [3/10], Batch [5/40], Loss: 1.6800\n",
      "Epoch [3/10], Batch [6/40], Loss: 1.5856\n",
      "Epoch [3/10], Batch [7/40], Loss: 1.4856\n",
      "Epoch [3/10], Batch [8/40], Loss: 1.3872\n",
      "Epoch [3/10], Batch [9/40], Loss: 1.2947\n",
      "Epoch [3/10], Batch [10/40], Loss: 0.3537\n",
      "Epoch [3/10], Batch [11/40], Loss: 0.3815\n",
      "Epoch [3/10], Batch [12/40], Loss: 0.4041\n",
      "Epoch [3/10], Batch [13/40], Loss: 0.4223\n",
      "Epoch [3/10], Batch [14/40], Loss: 0.4366\n",
      "Epoch [3/10], Batch [15/40], Loss: 0.4477\n",
      "Epoch [3/10], Batch [16/40], Loss: 0.4560\n",
      "Epoch [3/10], Batch [17/40], Loss: 0.4620\n",
      "Epoch [3/10], Batch [18/40], Loss: 0.4660\n",
      "Epoch [3/10], Batch [19/40], Loss: 0.4682\n",
      "Epoch [3/10], Batch [20/40], Loss: 0.4689\n",
      "Epoch [3/10], Batch [21/40], Loss: 0.4682\n",
      "Epoch [3/10], Batch [22/40], Loss: 0.4662\n",
      "Epoch [3/10], Batch [23/40], Loss: 0.4632\n",
      "Epoch [3/10], Batch [24/40], Loss: 0.4592\n",
      "Epoch [3/10], Batch [25/40], Loss: 0.4542\n",
      "Epoch [3/10], Batch [26/40], Loss: 0.4483\n",
      "Epoch [3/10], Batch [27/40], Loss: 0.4416\n",
      "Epoch [3/10], Batch [28/40], Loss: 0.4341\n",
      "Epoch [3/10], Batch [29/40], Loss: 0.4258\n",
      "Epoch [3/10], Batch [30/40], Loss: 0.4167\n",
      "Epoch [3/10], Batch [31/40], Loss: 0.4069\n",
      "Epoch [3/10], Batch [32/40], Loss: 0.3963\n",
      "Epoch [3/10], Batch [33/40], Loss: 0.3850\n",
      "Epoch [3/10], Batch [34/40], Loss: 0.3729\n",
      "Epoch [3/10], Batch [35/40], Loss: 0.3601\n",
      "Epoch [3/10], Batch [36/40], Loss: 0.3466\n",
      "Epoch [3/10], Batch [37/40], Loss: 0.3323\n",
      "Epoch [3/10], Batch [38/40], Loss: 0.3172\n",
      "Epoch [3/10], Batch [39/40], Loss: 0.3014\n",
      "Epoch [3/10], Batch [40/40], Loss: 0.2849\n",
      "Epoch [3/10], Batch [41/40], Loss: 0.2677\n",
      "Epoch [4/10], Batch [1/40], Loss: 1.4779\n",
      "Epoch [4/10], Batch [2/40], Loss: 1.5368\n",
      "Epoch [4/10], Batch [3/40], Loss: 1.5349\n",
      "Epoch [4/10], Batch [4/40], Loss: 1.5090\n",
      "Epoch [4/10], Batch [5/40], Loss: 1.4656\n",
      "Epoch [4/10], Batch [6/40], Loss: 1.4109\n",
      "Epoch [4/10], Batch [7/40], Loss: 1.3504\n",
      "Epoch [4/10], Batch [8/40], Loss: 1.2884\n",
      "Epoch [4/10], Batch [9/40], Loss: 1.2278\n",
      "Epoch [4/10], Batch [10/40], Loss: 0.3714\n",
      "Epoch [4/10], Batch [11/40], Loss: 0.3914\n",
      "Epoch [4/10], Batch [12/40], Loss: 0.4075\n",
      "Epoch [4/10], Batch [13/40], Loss: 0.4201\n",
      "Epoch [4/10], Batch [14/40], Loss: 0.4299\n",
      "Epoch [4/10], Batch [15/40], Loss: 0.4373\n",
      "Epoch [4/10], Batch [16/40], Loss: 0.4425\n",
      "Epoch [4/10], Batch [17/40], Loss: 0.4460\n",
      "Epoch [4/10], Batch [18/40], Loss: 0.4479\n",
      "Epoch [4/10], Batch [19/40], Loss: 0.4484\n",
      "Epoch [4/10], Batch [20/40], Loss: 0.4476\n",
      "Epoch [4/10], Batch [21/40], Loss: 0.4458\n",
      "Epoch [4/10], Batch [22/40], Loss: 0.4429\n",
      "Epoch [4/10], Batch [23/40], Loss: 0.4392\n",
      "Epoch [4/10], Batch [24/40], Loss: 0.4346\n",
      "Epoch [4/10], Batch [25/40], Loss: 0.4291\n",
      "Epoch [4/10], Batch [26/40], Loss: 0.4230\n",
      "Epoch [4/10], Batch [27/40], Loss: 0.4161\n",
      "Epoch [4/10], Batch [28/40], Loss: 0.4086\n",
      "Epoch [4/10], Batch [29/40], Loss: 0.4005\n",
      "Epoch [4/10], Batch [30/40], Loss: 0.3917\n",
      "Epoch [4/10], Batch [31/40], Loss: 0.3823\n",
      "Epoch [4/10], Batch [32/40], Loss: 0.3724\n",
      "Epoch [4/10], Batch [33/40], Loss: 0.3619\n",
      "Epoch [4/10], Batch [34/40], Loss: 0.3509\n",
      "Epoch [4/10], Batch [35/40], Loss: 0.3393\n",
      "Epoch [4/10], Batch [36/40], Loss: 0.3274\n",
      "Epoch [4/10], Batch [37/40], Loss: 0.3149\n",
      "Epoch [4/10], Batch [38/40], Loss: 0.3021\n",
      "Epoch [4/10], Batch [39/40], Loss: 0.2889\n",
      "Epoch [4/10], Batch [40/40], Loss: 0.2753\n",
      "Epoch [4/10], Batch [41/40], Loss: 0.2614\n",
      "Epoch [5/10], Batch [1/40], Loss: 1.4835\n",
      "Epoch [5/10], Batch [2/40], Loss: 1.5433\n",
      "Epoch [5/10], Batch [3/40], Loss: 1.5448\n",
      "Epoch [5/10], Batch [4/40], Loss: 1.5255\n",
      "Epoch [5/10], Batch [5/40], Loss: 1.4897\n",
      "Epoch [5/10], Batch [6/40], Loss: 1.4422\n",
      "Epoch [5/10], Batch [7/40], Loss: 1.3875\n",
      "Epoch [5/10], Batch [8/40], Loss: 1.3294\n",
      "Epoch [5/10], Batch [9/40], Loss: 1.2712\n",
      "Epoch [5/10], Batch [10/40], Loss: 0.3521\n",
      "Epoch [5/10], Batch [11/40], Loss: 0.3708\n",
      "Epoch [5/10], Batch [12/40], Loss: 0.3860\n",
      "Epoch [5/10], Batch [13/40], Loss: 0.3981\n",
      "Epoch [5/10], Batch [14/40], Loss: 0.4075\n",
      "Epoch [5/10], Batch [15/40], Loss: 0.4146\n",
      "Epoch [5/10], Batch [16/40], Loss: 0.4197\n",
      "Epoch [5/10], Batch [17/40], Loss: 0.4231\n",
      "Epoch [5/10], Batch [18/40], Loss: 0.4250\n",
      "Epoch [5/10], Batch [19/40], Loss: 0.4255\n",
      "Epoch [5/10], Batch [20/40], Loss: 0.4248\n",
      "Epoch [5/10], Batch [21/40], Loss: 0.4231\n",
      "Epoch [5/10], Batch [22/40], Loss: 0.4204\n",
      "Epoch [5/10], Batch [23/40], Loss: 0.4168\n",
      "Epoch [5/10], Batch [24/40], Loss: 0.4124\n",
      "Epoch [5/10], Batch [25/40], Loss: 0.4072\n",
      "Epoch [5/10], Batch [26/40], Loss: 0.4012\n",
      "Epoch [5/10], Batch [27/40], Loss: 0.3946\n",
      "Epoch [5/10], Batch [28/40], Loss: 0.3874\n",
      "Epoch [5/10], Batch [29/40], Loss: 0.3796\n",
      "Epoch [5/10], Batch [30/40], Loss: 0.3712\n",
      "Epoch [5/10], Batch [31/40], Loss: 0.3622\n",
      "Epoch [5/10], Batch [32/40], Loss: 0.3527\n",
      "Epoch [5/10], Batch [33/40], Loss: 0.3428\n",
      "Epoch [5/10], Batch [34/40], Loss: 0.3324\n",
      "Epoch [5/10], Batch [35/40], Loss: 0.3216\n",
      "Epoch [5/10], Batch [36/40], Loss: 0.3103\n",
      "Epoch [5/10], Batch [37/40], Loss: 0.2988\n",
      "Epoch [5/10], Batch [38/40], Loss: 0.2869\n",
      "Epoch [5/10], Batch [39/40], Loss: 0.2747\n",
      "Epoch [5/10], Batch [40/40], Loss: 0.2624\n",
      "Epoch [5/10], Batch [41/40], Loss: 0.2498\n",
      "Epoch [6/10], Batch [1/40], Loss: 1.5030\n",
      "Epoch [6/10], Batch [2/40], Loss: 1.5793\n",
      "Epoch [6/10], Batch [3/40], Loss: 1.5809\n",
      "Epoch [6/10], Batch [4/40], Loss: 1.5621\n",
      "Epoch [6/10], Batch [5/40], Loss: 1.5268\n",
      "Epoch [6/10], Batch [6/40], Loss: 1.4793\n",
      "Epoch [6/10], Batch [7/40], Loss: 1.4241\n",
      "Epoch [6/10], Batch [8/40], Loss: 1.3651\n",
      "Epoch [6/10], Batch [9/40], Loss: 1.3053\n",
      "Epoch [6/10], Batch [10/40], Loss: 0.3387\n",
      "Epoch [6/10], Batch [11/40], Loss: 0.3573\n",
      "Epoch [6/10], Batch [12/40], Loss: 0.3724\n",
      "Epoch [6/10], Batch [13/40], Loss: 0.3846\n",
      "Epoch [6/10], Batch [14/40], Loss: 0.3941\n",
      "Epoch [6/10], Batch [15/40], Loss: 0.4014\n",
      "Epoch [6/10], Batch [16/40], Loss: 0.4067\n",
      "Epoch [6/10], Batch [17/40], Loss: 0.4103\n",
      "Epoch [6/10], Batch [18/40], Loss: 0.4125\n",
      "Epoch [6/10], Batch [19/40], Loss: 0.4133\n",
      "Epoch [6/10], Batch [20/40], Loss: 0.4130\n",
      "Epoch [6/10], Batch [21/40], Loss: 0.4116\n",
      "Epoch [6/10], Batch [22/40], Loss: 0.4093\n",
      "Epoch [6/10], Batch [23/40], Loss: 0.4060\n",
      "Epoch [6/10], Batch [24/40], Loss: 0.4020\n",
      "Epoch [6/10], Batch [25/40], Loss: 0.3973\n",
      "Epoch [6/10], Batch [26/40], Loss: 0.3918\n",
      "Epoch [6/10], Batch [27/40], Loss: 0.3857\n",
      "Epoch [6/10], Batch [28/40], Loss: 0.3790\n",
      "Epoch [6/10], Batch [29/40], Loss: 0.3717\n",
      "Epoch [6/10], Batch [30/40], Loss: 0.3639\n",
      "Epoch [6/10], Batch [31/40], Loss: 0.3556\n",
      "Epoch [6/10], Batch [32/40], Loss: 0.3468\n",
      "Epoch [6/10], Batch [33/40], Loss: 0.3375\n",
      "Epoch [6/10], Batch [34/40], Loss: 0.3278\n",
      "Epoch [6/10], Batch [35/40], Loss: 0.3177\n",
      "Epoch [6/10], Batch [36/40], Loss: 0.3073\n",
      "Epoch [6/10], Batch [37/40], Loss: 0.2965\n",
      "Epoch [6/10], Batch [38/40], Loss: 0.2854\n",
      "Epoch [6/10], Batch [39/40], Loss: 0.2741\n",
      "Epoch [6/10], Batch [40/40], Loss: 0.2625\n",
      "Epoch [6/10], Batch [41/40], Loss: 0.2508\n",
      "Epoch [7/10], Batch [1/40], Loss: 1.4862\n",
      "Epoch [7/10], Batch [2/40], Loss: 1.5714\n",
      "Epoch [7/10], Batch [3/40], Loss: 1.5732\n",
      "Epoch [7/10], Batch [4/40], Loss: 1.5560\n",
      "Epoch [7/10], Batch [5/40], Loss: 1.5232\n",
      "Epoch [7/10], Batch [6/40], Loss: 1.4789\n",
      "Epoch [7/10], Batch [7/40], Loss: 1.4270\n",
      "Epoch [7/10], Batch [8/40], Loss: 1.3712\n",
      "Epoch [7/10], Batch [9/40], Loss: 1.3144\n",
      "Epoch [7/10], Batch [10/40], Loss: 0.3341\n",
      "Epoch [7/10], Batch [11/40], Loss: 0.3516\n",
      "Epoch [7/10], Batch [12/40], Loss: 0.3658\n",
      "Epoch [7/10], Batch [13/40], Loss: 0.3773\n",
      "Epoch [7/10], Batch [14/40], Loss: 0.3863\n",
      "Epoch [7/10], Batch [15/40], Loss: 0.3931\n",
      "Epoch [7/10], Batch [16/40], Loss: 0.3981\n",
      "Epoch [7/10], Batch [17/40], Loss: 0.4016\n",
      "Epoch [7/10], Batch [18/40], Loss: 0.4035\n",
      "Epoch [7/10], Batch [19/40], Loss: 0.4043\n",
      "Epoch [7/10], Batch [20/40], Loss: 0.4039\n",
      "Epoch [7/10], Batch [21/40], Loss: 0.4025\n",
      "Epoch [7/10], Batch [22/40], Loss: 0.4003\n",
      "Epoch [7/10], Batch [23/40], Loss: 0.3971\n",
      "Epoch [7/10], Batch [24/40], Loss: 0.3933\n",
      "Epoch [7/10], Batch [25/40], Loss: 0.3887\n",
      "Epoch [7/10], Batch [26/40], Loss: 0.3834\n",
      "Epoch [7/10], Batch [27/40], Loss: 0.3776\n",
      "Epoch [7/10], Batch [28/40], Loss: 0.3712\n",
      "Epoch [7/10], Batch [29/40], Loss: 0.3642\n",
      "Epoch [7/10], Batch [30/40], Loss: 0.3567\n",
      "Epoch [7/10], Batch [31/40], Loss: 0.3488\n",
      "Epoch [7/10], Batch [32/40], Loss: 0.3403\n",
      "Epoch [7/10], Batch [33/40], Loss: 0.3315\n",
      "Epoch [7/10], Batch [34/40], Loss: 0.3222\n",
      "Epoch [7/10], Batch [35/40], Loss: 0.3126\n",
      "Epoch [7/10], Batch [36/40], Loss: 0.3026\n",
      "Epoch [7/10], Batch [37/40], Loss: 0.2924\n",
      "Epoch [7/10], Batch [38/40], Loss: 0.2818\n",
      "Epoch [7/10], Batch [39/40], Loss: 0.2710\n",
      "Epoch [7/10], Batch [40/40], Loss: 0.2600\n",
      "Epoch [7/10], Batch [41/40], Loss: 0.2488\n",
      "Epoch [8/10], Batch [1/40], Loss: 1.4946\n",
      "Epoch [8/10], Batch [2/40], Loss: 1.5758\n",
      "Epoch [8/10], Batch [3/40], Loss: 1.5771\n",
      "Epoch [8/10], Batch [4/40], Loss: 1.5602\n",
      "Epoch [8/10], Batch [5/40], Loss: 1.5285\n",
      "Epoch [8/10], Batch [6/40], Loss: 1.4857\n",
      "Epoch [8/10], Batch [7/40], Loss: 1.4355\n",
      "Epoch [8/10], Batch [8/40], Loss: 1.3814\n",
      "Epoch [8/10], Batch [9/40], Loss: 1.3261\n",
      "Epoch [8/10], Batch [10/40], Loss: 0.3289\n",
      "Epoch [8/10], Batch [11/40], Loss: 0.3457\n",
      "Epoch [8/10], Batch [12/40], Loss: 0.3595\n",
      "Epoch [8/10], Batch [13/40], Loss: 0.3705\n",
      "Epoch [8/10], Batch [14/40], Loss: 0.3792\n",
      "Epoch [8/10], Batch [15/40], Loss: 0.3858\n",
      "Epoch [8/10], Batch [16/40], Loss: 0.3907\n",
      "Epoch [8/10], Batch [17/40], Loss: 0.3940\n",
      "Epoch [8/10], Batch [18/40], Loss: 0.3960\n",
      "Epoch [8/10], Batch [19/40], Loss: 0.3968\n",
      "Epoch [8/10], Batch [20/40], Loss: 0.3965\n",
      "Epoch [8/10], Batch [21/40], Loss: 0.3953\n",
      "Epoch [8/10], Batch [22/40], Loss: 0.3931\n",
      "Epoch [8/10], Batch [23/40], Loss: 0.3902\n",
      "Epoch [8/10], Batch [24/40], Loss: 0.3866\n",
      "Epoch [8/10], Batch [25/40], Loss: 0.3823\n",
      "Epoch [8/10], Batch [26/40], Loss: 0.3773\n",
      "Epoch [8/10], Batch [27/40], Loss: 0.3718\n",
      "Epoch [8/10], Batch [28/40], Loss: 0.3657\n",
      "Epoch [8/10], Batch [29/40], Loss: 0.3592\n",
      "Epoch [8/10], Batch [30/40], Loss: 0.3521\n",
      "Epoch [8/10], Batch [31/40], Loss: 0.3446\n",
      "Epoch [8/10], Batch [32/40], Loss: 0.3367\n",
      "Epoch [8/10], Batch [33/40], Loss: 0.3283\n",
      "Epoch [8/10], Batch [34/40], Loss: 0.3196\n",
      "Epoch [8/10], Batch [35/40], Loss: 0.3106\n",
      "Epoch [8/10], Batch [36/40], Loss: 0.3012\n",
      "Epoch [8/10], Batch [37/40], Loss: 0.2915\n",
      "Epoch [8/10], Batch [38/40], Loss: 0.2815\n",
      "Epoch [8/10], Batch [39/40], Loss: 0.2714\n",
      "Epoch [8/10], Batch [40/40], Loss: 0.2609\n",
      "Epoch [8/10], Batch [41/40], Loss: 0.2504\n",
      "Epoch [9/10], Batch [1/40], Loss: 1.4749\n",
      "Epoch [9/10], Batch [2/40], Loss: 1.5668\n",
      "Epoch [9/10], Batch [3/40], Loss: 1.5683\n",
      "Epoch [9/10], Batch [4/40], Loss: 1.5527\n",
      "Epoch [9/10], Batch [5/40], Loss: 1.5230\n",
      "Epoch [9/10], Batch [6/40], Loss: 1.4827\n",
      "Epoch [9/10], Batch [7/40], Loss: 1.4352\n",
      "Epoch [9/10], Batch [8/40], Loss: 1.3840\n",
      "Epoch [9/10], Batch [9/40], Loss: 1.3315\n",
      "Epoch [9/10], Batch [10/40], Loss: 0.3259\n",
      "Epoch [9/10], Batch [11/40], Loss: 0.3418\n",
      "Epoch [9/10], Batch [12/40], Loss: 0.3547\n",
      "Epoch [9/10], Batch [13/40], Loss: 0.3651\n",
      "Epoch [9/10], Batch [14/40], Loss: 0.3732\n",
      "Epoch [9/10], Batch [15/40], Loss: 0.3795\n",
      "Epoch [9/10], Batch [16/40], Loss: 0.3840\n",
      "Epoch [9/10], Batch [17/40], Loss: 0.3871\n",
      "Epoch [9/10], Batch [18/40], Loss: 0.3890\n",
      "Epoch [9/10], Batch [19/40], Loss: 0.3896\n",
      "Epoch [9/10], Batch [20/40], Loss: 0.3893\n",
      "Epoch [9/10], Batch [21/40], Loss: 0.3880\n",
      "Epoch [9/10], Batch [22/40], Loss: 0.3859\n",
      "Epoch [9/10], Batch [23/40], Loss: 0.3831\n",
      "Epoch [9/10], Batch [24/40], Loss: 0.3795\n",
      "Epoch [9/10], Batch [25/40], Loss: 0.3753\n",
      "Epoch [9/10], Batch [26/40], Loss: 0.3705\n",
      "Epoch [9/10], Batch [27/40], Loss: 0.3652\n",
      "Epoch [9/10], Batch [28/40], Loss: 0.3593\n",
      "Epoch [9/10], Batch [29/40], Loss: 0.3530\n",
      "Epoch [9/10], Batch [30/40], Loss: 0.3462\n",
      "Epoch [9/10], Batch [31/40], Loss: 0.3390\n",
      "Epoch [9/10], Batch [32/40], Loss: 0.3313\n",
      "Epoch [9/10], Batch [33/40], Loss: 0.3233\n",
      "Epoch [9/10], Batch [34/40], Loss: 0.3149\n",
      "Epoch [9/10], Batch [35/40], Loss: 0.3062\n",
      "Epoch [9/10], Batch [36/40], Loss: 0.2971\n",
      "Epoch [9/10], Batch [37/40], Loss: 0.2878\n",
      "Epoch [9/10], Batch [38/40], Loss: 0.2782\n",
      "Epoch [9/10], Batch [39/40], Loss: 0.2684\n",
      "Epoch [9/10], Batch [40/40], Loss: 0.2583\n",
      "Epoch [9/10], Batch [41/40], Loss: 0.2481\n",
      "Epoch [10/10], Batch [1/40], Loss: 1.4881\n",
      "Epoch [10/10], Batch [2/40], Loss: 1.5731\n",
      "Epoch [10/10], Batch [3/40], Loss: 1.5741\n",
      "Epoch [10/10], Batch [4/40], Loss: 1.5583\n",
      "Epoch [10/10], Batch [5/40], Loss: 1.5287\n",
      "Epoch [10/10], Batch [6/40], Loss: 1.4888\n",
      "Epoch [10/10], Batch [7/40], Loss: 1.4421\n",
      "Epoch [10/10], Batch [8/40], Loss: 1.3917\n",
      "Epoch [10/10], Batch [9/40], Loss: 1.3402\n",
      "Epoch [10/10], Batch [10/40], Loss: 0.3221\n",
      "Epoch [10/10], Batch [11/40], Loss: 0.3375\n",
      "Epoch [10/10], Batch [12/40], Loss: 0.3500\n",
      "Epoch [10/10], Batch [13/40], Loss: 0.3600\n",
      "Epoch [10/10], Batch [14/40], Loss: 0.3679\n",
      "Epoch [10/10], Batch [15/40], Loss: 0.3739\n",
      "Epoch [10/10], Batch [16/40], Loss: 0.3783\n",
      "Epoch [10/10], Batch [17/40], Loss: 0.3813\n",
      "Epoch [10/10], Batch [18/40], Loss: 0.3831\n",
      "Epoch [10/10], Batch [19/40], Loss: 0.3837\n",
      "Epoch [10/10], Batch [20/40], Loss: 0.3834\n",
      "Epoch [10/10], Batch [21/40], Loss: 0.3822\n",
      "Epoch [10/10], Batch [22/40], Loss: 0.3802\n",
      "Epoch [10/10], Batch [23/40], Loss: 0.3775\n",
      "Epoch [10/10], Batch [24/40], Loss: 0.3741\n",
      "Epoch [10/10], Batch [25/40], Loss: 0.3701\n",
      "Epoch [10/10], Batch [26/40], Loss: 0.3655\n",
      "Epoch [10/10], Batch [27/40], Loss: 0.3605\n",
      "Epoch [10/10], Batch [28/40], Loss: 0.3549\n",
      "Epoch [10/10], Batch [29/40], Loss: 0.3488\n",
      "Epoch [10/10], Batch [30/40], Loss: 0.3424\n",
      "Epoch [10/10], Batch [31/40], Loss: 0.3355\n",
      "Epoch [10/10], Batch [32/40], Loss: 0.3282\n",
      "Epoch [10/10], Batch [33/40], Loss: 0.3205\n",
      "Epoch [10/10], Batch [34/40], Loss: 0.3125\n",
      "Epoch [10/10], Batch [35/40], Loss: 0.3042\n",
      "Epoch [10/10], Batch [36/40], Loss: 0.2956\n",
      "Epoch [10/10], Batch [37/40], Loss: 0.2867\n",
      "Epoch [10/10], Batch [38/40], Loss: 0.2775\n",
      "Epoch [10/10], Batch [39/40], Loss: 0.2681\n",
      "Epoch [10/10], Batch [40/40], Loss: 0.2585\n",
      "Epoch [10/10], Batch [41/40], Loss: 0.2487\n"
     ]
    }
   ],
   "source": [
    "logging = True\n",
    "filename = 'r'+ str(num_layers)+ '_f'+ str(num_fclayers)+ '_'+ last_func+ '_h'+ str(hidden_size)+ '_b'+ str(batch_size)+ '_l'+ str(learning_rate)+ '_e'+ str(num_epochs) \n",
    "filepath = \"./logs/\" +filename+\".txt\"\n",
    "if logging:\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('Start logging loss' + '\\n')\n",
    "\n",
    "# set train mode \n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "total_step = len(tensor_train_data) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(tensor_train_data), batch_size):\n",
    "        \n",
    "        # Mini-batch data\n",
    "        batch_inputs = tensor_train_data[i:i+batch_size, :, :].float()\n",
    "        batch_labels = tensor_train_labels[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        loss = loss_func(outputs.float(), batch_labels.float())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics (adjust to modolu)\n",
    "        if (i+1) % 1 == 0:\n",
    "            message = 'Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i//batch_size+1, total_step, loss.item())\n",
    "            print(message)\n",
    "            if logging:\n",
    "                # log results\n",
    "                with open(filepath, 'a') as f:\n",
    "                    f.write(message+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cf6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6006600660066007\n"
     ]
    }
   ],
   "source": [
    "# set evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tensor_test_data.float())\n",
    "\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "acc= (preds == tensor_test_labels.argmax(1)).sum().item() / tensor_test_labels.shape[0]\n",
    "\n",
    "\n",
    "print(\"accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "454a425d5820fe3d5235a5c44cbe7f0b11db398b8989037c90b6a1734bb532fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
