{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efe4fc-1340-4ad5-ab15-25de6bf4ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "        \n",
    "    return conn[db]\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    db =_connect_mongo(db=db, host=host, port=port, username=username, password=password)\n",
    "    cursor = db[collection].find(query)\n",
    "        \n",
    "    return list(cursor)\n",
    "\n",
    "def _build_vector(event):\n",
    "    # vector_len = 290\n",
    "    # f_vector = np.zeros(vectror_len)\n",
    "    \n",
    "    m_country = assign_country_mapper()\n",
    "    m_event = assign_event_type_mapper()\n",
    "    m_agent_os = assign_agent_os_mapper()\n",
    "    m_agent_name = assign_agent_name_mapper()\n",
    "    \n",
    "    f_vector_evty = np.zeros(4)\n",
    "    f_vector_evty[m_event[event['type']]] = 1\n",
    "    \n",
    "    \n",
    "    dt = datetime.utcfromtimestamp(event['timestamp'])\n",
    "    minutes = (dt.hour * 60) + dt.minute\n",
    "    day = dt.weekday()\n",
    "    f_vector_dt = np.zeros(2)\n",
    "    f_vector_dt[0] = round(minutes / 1439, 5)\n",
    "    f_vector_dt[1] = round(day / 6, 5)\n",
    "    \n",
    "    latitude = event['geoip']['latitude'][0]\n",
    "    longitude = event['geoip']['longitude'][0]\n",
    "    f_vector_geo = np.zeros(2)\n",
    "    f_vector_geo[0]= round((latitude + 90) / 180,5)\n",
    "    f_vector_geo[1]= round((longitude + 180) / 360, 5)\n",
    "    \n",
    "    f_vector_agos = np.zeros(8)\n",
    "    os = event['agent']['os'][0]\n",
    "    if os in m_agent_os:\n",
    "        f_vector_agos[m_agent_os[os]] = 1\n",
    "    else:\n",
    "        f_vector_agos[-1] = 1\n",
    "        \n",
    "    f_vector_agna = np.zeros(26)\n",
    "    name = event['agent']['name'][0]\n",
    "    if name in m_agent_name:\n",
    "        f_vector_agna[m_agent_name[name]] = 1\n",
    "    else:\n",
    "        f_vector_agna[-1] = 1\n",
    "        \n",
    "    f_vector_country = np.zeros(248)\n",
    "    country = event['geoip']['country'][0]\n",
    "    if country in m_country:\n",
    "        f_vector_country[m_country[country]] = 1\n",
    "    else:\n",
    "        f_vector_country[-1] = 1\n",
    "    \n",
    "    # without geo and without country features\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_agos, f_vector_agna])\n",
    "\n",
    "    # without country feature\n",
    "    # return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna])\n",
    "    \n",
    "    # all features\n",
    "    return np.concatenate([f_vector_evty, f_vector_dt, f_vector_geo, f_vector_agos, f_vector_agna, f_vector_country])\n",
    "\n",
    "def vectorize(json):\n",
    "    events = sorted(np.concatenate([json['connects'], json['plays'], json['h5liveStats'],json['closes']]), key=lambda d: d['timestamp'])\n",
    "    vector_list = []\n",
    "    if len(events) == 0:\n",
    "        return None\n",
    "    for event in events:\n",
    "        vector_list.append(_build_vector(event))\n",
    "    return np.stack(vector_list)\n",
    "\n",
    "def add_event_types(json):\n",
    "    if 'rtmpStats' in json:\n",
    "        rtmpStats = json['rtmpStats']\n",
    "        for x in rtmpStats:\n",
    "            x['type']='rtmp'\n",
    "    \n",
    "    if 'connects' in json:\n",
    "        connects = json['connects'] \n",
    "        for x in connects:\n",
    "            x['type']='connect'\n",
    "        \n",
    "    if 'plays' in json:\n",
    "        plays = json['plays']\n",
    "        for x in plays:\n",
    "            x['type']='play'\n",
    "            \n",
    "    if 'h5liveStats' in json:\n",
    "        h5liveStats = json['h5liveStats']\n",
    "        for x in h5liveStats:\n",
    "            x['type']='h5live'\n",
    "    \n",
    "    if 'closes' in json:\n",
    "        closes = json['closes']\n",
    "        for x in closes:\n",
    "            x['type']='close'  \n",
    "    \n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed942924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def assign_country_mapper():\n",
    "    with open('../../src/util/country_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "\n",
    "def assign_event_type_mapper():\n",
    "    with open('../../src/util/event_type_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_os_mapper():\n",
    "    with open('../../src/util/agent_os_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "def assign_agent_name_mapper():\n",
    "    with open('../../src/util/agent_name_mapper.json') as f:\n",
    "        d = json.load(f)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from MongoDB\n",
    "misuses = read_mongo('dataset1', 'misuse_data')\n",
    "regulars = read_mongo('dataset1', 'regular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd324fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_misuses = []\n",
    "v_regulars = []\n",
    "\n",
    "for x in misuses:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_misuses.append(y)\n",
    "\n",
    "for x in regulars:\n",
    "    x = add_event_types(x)\n",
    "    y = vectorize(x)\n",
    "    if y is not None:\n",
    "        # print(y.shape)\n",
    "        v_regulars.append(y)\n",
    "\n",
    "print('Misuses: ' + str(len(misuses)), 'Regulars: ' + str(len(regulars)))\n",
    "print('Valid Misuses: ' + str(len(v_misuses)), 'Valid Regulars: ' + str(len(v_regulars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ade275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = (144,496)\n",
    "im = torch.tensor([[1,0], [0,1]])\n",
    "\n",
    "X_train_misuse = v_misuses[:train_size[0]:1]\n",
    "X_train_regular = v_regulars[:train_size[1]:1]\n",
    "X_train_final = X_train_misuse + X_train_regular\n",
    "\n",
    "y_train = np.zeros_like(list(range(len(X_train_final))))\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if i >= len(X_train_misuse):\n",
    "        y_train[i] = 1\n",
    "\n",
    "max_length_training = max([arr.shape[0] for arr in X_train_final])\n",
    "padded_train_data = np.array([np.pad(arr,((0, max_length_training - arr.shape[0]), (0,0)), mode='constant') for arr in X_train_final])\n",
    "\n",
    "tensor_train_data = Variable(torch.tensor(padded_train_data, dtype=torch.float64))\n",
    "tensor_train_labels = Variable(im[torch.tensor(y_train)])\n",
    "\n",
    "\n",
    "X_test_misuse = v_misuses[train_size[0]::1]\n",
    "X_test_regular = v_regulars[train_size[1]::1]\n",
    "X_test_final = X_test_misuse + X_test_regular\n",
    "\n",
    "y_test = np.zeros_like(list(range(len(X_test_final))))\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if i >= len(X_test_misuse):\n",
    "        y_test[i] = 1\n",
    "\n",
    "max_length_testing = max([arr.shape[0] for arr in X_test_final])\n",
    "padded_test_data = np.array([np.pad(arr,((0, max_length_testing - arr.shape[0]), (0,0)), mode='constant') for arr in X_test_final])\n",
    "\n",
    "tensor_test_data = Variable(torch.tensor(padded_test_data, dtype=torch.float64))\n",
    "tensor_test_labels = Variable(im[torch.tensor(y_test)])\n",
    "\n",
    "print('Training: ', tensor_train_data.shape, ' ', tensor_train_labels.shape)\n",
    "print('Testing: ', tensor_test_data.shape, ' ', tensor_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = tensor_train_data.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.003\n",
    "batch_size = 16\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, seq_length):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # use last output of the last lstm layer\n",
    "        out = lstm_out[:, -1, :]\n",
    "    \n",
    "        # first fully connected layer\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        # second fully connected layer\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        # third fully connected layer\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        out = self.soft(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, tensor_train_data.shape[0]).to(device)\n",
    "\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#define for filename\n",
    "num_fclayers = 3\n",
    "last_func = 'sigm'\n",
    "l_func = 'BCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = True\n",
    "filename = 'r'+ str(num_layers)+ '_f'+ str(num_fclayers)+ '_'+ last_func+ '_h'+ str(hidden_size)+ '_b'+ str(batch_size)+ '_l'+ str(learning_rate)+ '_e'+ str(num_epochs) \n",
    "filepath = \"./results/\" +filename+\".txt\"\n",
    "if logging:\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('Start logging loss' + '\\n')\n",
    "\n",
    "# set train mode \n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "total_step = len(tensor_train_data) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(tensor_train_data), batch_size):\n",
    "        \n",
    "        # Mini-batch data\n",
    "        batch_inputs = tensor_train_data[i:i+batch_size, :, :].float()\n",
    "        batch_labels = tensor_train_labels[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        loss = loss_func(outputs.float(), batch_labels.float())\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics (adjust to modolu)\n",
    "        if (i+1) % 1 == 0:\n",
    "            message = 'Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i//batch_size+1, total_step, loss.item())\n",
    "            print(message)\n",
    "            if logging:\n",
    "                # log results\n",
    "                with open(filepath, 'a') as f:\n",
    "                    f.write(message+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(tensor_test_data.float())\n",
    "\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "acc= (preds == tensor_test_labels.argmax(1)).sum().item() / tensor_test_labels.shape[0]\n",
    "\n",
    "\n",
    "print(\"accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "454a425d5820fe3d5235a5c44cbe7f0b11db398b8989037c90b6a1734bb532fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
